{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.0)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading numpy-1.24.3-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.56.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Downloading numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.8 MB 330.3 kB/s eta 0:00:45\n",
      "   ---------------------------------------- 0.1/14.8 MB 819.2 kB/s eta 0:00:18\n",
      "    --------------------------------------- 0.3/14.8 MB 1.9 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.5/14.8 MB 2.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/14.8 MB 3.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.0/14.8 MB 3.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.3/14.8 MB 3.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.5/14.8 MB 3.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.7/14.8 MB 3.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.9/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.0/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.3/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.4/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.8/14.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.9/14.8 MB 4.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.2/14.8 MB 4.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.4/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.6/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.9/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.4/14.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.6/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.8/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.0/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.2/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.4/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.7/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.9/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.1/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.4/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.6/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.8/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.0/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.2/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.4/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.5/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.8/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.9/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.2/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.5/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.6/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.0/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.1/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.3/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.7/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.9/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.2/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.3/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.4/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.7/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.0/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.2/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.5/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.6/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.0/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.2/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.4/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.7/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.8/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.2/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.4/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.5/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.8/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.0/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.3/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.8/14.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.8/14.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.8/14.8 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: typing-extensions, numpy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.1\n",
      "    Uninstalling numpy-1.25.1:\n",
      "      Successfully uninstalled numpy-1.25.1\n",
      "Successfully installed numpy-1.24.3 typing-extensions-4.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.7.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.18.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np  \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Alpaca_Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stock \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAlpaca_Data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mipynb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_data\n\u001b[0;32m      4\u001b[0m create_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1050\u001b[39m)   \u001b[38;5;66;03m# 700 works  1050 works\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspy done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Alpaca_Data'"
     ]
    }
   ],
   "source": [
    "stock = \"AMD\"\n",
    "#from Alpaca_Data.ipynb import create_data\n",
    "\n",
    "# create_data(\"SPY\", 1050)   # 700 works  1050 works\n",
    "# print(\"spy done\")\n",
    "# create_data(\"\", 1050) \n",
    "# print(\"both good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.44, 0.24, 0.93, 0.35, -0.13, -0.05, 0.54, 0.47, 0.07, -0.06, 0.73, 0.37, -0.23, 0.72, -0.36, 0.08, -0.09, 0.3, -1.31, -0.15, 0.39, 0.09, 0.12, -0.22, 0.75, 0.42, -0.32, -0.19, 0.54, 0.37, -0.08, 0.55, 0.03, 0.17, -0.21, -0.21, -1.29, -0.01, 0.38, 0.86, -0.23, 0.77, 0.38, -0.86, -0.49, -1.09, -0.28, -2.07, -0.68, 0.45, 0.12, -0.23, -0.08, 0.77, -2.0, 0.33, 0.12, -0.32, -0.03, 0.28, -0.46, -0.01, -0.17, -0.23, 0.91, 0.7, -0.22, -0.07, -0.39, -0.22, 0.65, 0.16, -0.73, 0.33, -0.16, -0.6, 0.05, 0.76, 0.0, 0.18, 0.05, -0.45, -0.28, 0.69, 0.36, -0.19, -0.44, 0.54, 0.36, -0.09, -0.04, 0.47, 0.04, 0.36, 0.09, -0.23, 0.96, 0.94, -1.17, 0.07, 0.73, -0.04, 0.09, -0.5, 0.18, -0.12, 1.0, 0.52, 0.1, -0.07, -0.24, -0.26, 0.51, 0.43, 1.32, 0.09, -0.22, -0.35, 0.1, -0.25, -0.04, 0.23, 0.33, -0.04, 0.29, -1.2, 0.49, 0.21, -0.03, -0.1, 1.29, 0.54, 0.5, 0.6, 0.29, -0.88, 0.29, 0.24, 0.73, 0.2, -0.56, 0.19, -0.04, 0.05, 0.01, 0.02, 0.83, 0.12, 0.22, -0.1, 0.54, 0.22, 1.06, -1.05, -0.07, 0.28, 0.05, 0.36, 0.98, 0.83, 0.49, 0.49, -0.85, -0.94, -1.03, 0.23, 0.2, -1.12, -1.05, -0.92, 0.74, 0.51, -0.85, -0.75, 0.16, 0.37, 1.1, 1.79, 0.03, 0.61, -0.82, 0.16, -0.97, 0.71, -0.24, -0.75, 0.71, -0.47, -0.99, -1.21, 0.01, 0.13, -0.84, 0.29, 0.06, -0.21, 0.05, 0.14, 0.39, -0.49, -0.33, -0.44, -0.29, 0.33, 1.47, 0.12, 0.29, -1.75, 0.86, -0.69, 0.41, 0.72, -1.1, -0.64, -0.76, 0.77, 0.38, -0.51, -0.73, 0.15, 0.45, -0.87, 0.18, -0.69, 0.05, 0.08, 0.23, -1.42, 0.23, 0.33, 0.18, -0.39, -0.44, 0.04, 0.82, 0.38, -0.27, 0.37, -0.08, 0.46, 0.34, -0.02, 0.05, 0.28, 0.2, 0.42, 0.49, 0.31, 0.88, -0.27, 0.06, 0.59, -0.28, -0.06, -0.8, 1.44, 0.04, 0.31, 0.67, -0.01, 0.59, -0.44, 0.32, -0.28, 0.81, 0.89, -0.1, -0.44, 1.13, -0.02, -0.32, -0.72, 0.04, -0.37, 1.04, 0.7, -0.39, 0.19, -0.44, 0.04, -0.25, -0.05, -0.06, 0.91, -0.44, -0.81, -0.95, 0.01, 1.08, 1.33, -0.58, -1.1, 0.16, 0.0, 0.16, 0.47, -0.33, 0.38, -0.08, 1.05, -0.93, -0.13, 0.74, 0.59, -0.08, -0.72, 0.51, 1.17, -0.1, 0.36, -0.04, -0.04, -0.41, 1.0, -0.49, -1.66, 0.42, 0.75, -0.82, 2.4, 0.88, 0.31, 0.93, -1.3, -2.05, 0.13, -1.52, -0.14, 1.12, 1.31, -0.17, -0.24, -0.54, 0.24, -0.22, -0.25, -0.98, 0.3, -0.12, 0.88, 0.34, 1.01, 0.54, -1.77, -0.6, 1.55, 0.01, 0.18, 0.46, 1.38, 1.33, -0.55, 0.5, 0.4, 1.11, 0.33, 0.99, 1.48, -0.18, -1.88, -0.18, 1.24, 0.07, 0.84, 0.86, -0.64, 1.43, -0.61, 0.15, -0.92, 0.47, 1.0, -1.22, -0.36, 0.86, -0.61, 0.78, 0.35, -0.9, -0.5, -1.18, -0.55, -2.01, 1.23, -0.42, 0.28, 0.06, -1.4, -1.08, 1.16, -0.34, 3.08, -0.21, -0.8, 0.12, 0.72, 0.82, -0.01, -0.43, 0.97, -0.34, -0.66, -0.39, 0.74, 1.71, -1.53, 0.23, 0.59, -0.17, -0.12, -2.35, -1.44, -0.06, 2.41, -0.81, 0.1, 1.62, 0.79, 2.51, -0.71, -0.13, -1.07, 0.77, -2.96, 4.8, -0.45, -0.14, -1.08, -1.67, -0.64, 0.99, 1.5, 1.53, -1.28, -1.1, 1.69, -1.26, -0.57, -0.71, -0.63, -2.26, -0.25, 1.65, 0.37, -0.72, 0.03, -2.17, 0.54, 0.96, 0.96, 1.26, 1.88, -0.6, -2.01, 0.9, -1.19, -1.4, 0.11, -3.36, 1.03, 0.38, -0.13, -0.89, -0.67, 0.24, 0.17, 0.46, 0.96, 1.2, -0.71, 0.29, -0.21, -0.54, 0.93, -0.05, 1.01, -0.26, 0.4, 1.08, 1.03, 1.69, -0.75, -0.05, -0.96, 1.17, 0.59, 1.6, -1.66, 0.67, 1.15, 0.99, -0.74, -0.42, 0.36, 1.0, 0.3, 1.62, 1.24, 0.27, -0.23, -2.45, -0.63, 2.28, 0.38, 1.02, 0.86, 0.1, -1.04, 0.49, -0.79, -1.28, -1.29, -1.93, -0.65, 1.87, -0.72, -0.45, 1.93, -1.34, -0.15, 1.8, 1.67, 1.29, 0.34, 1.04, -0.92, 0.22, -2.88, 0.44, 0.03, 1.26, 0.76, -1.34, -1.34, -1.71, 0.06, -2.53, 2.87, 0.33, 0.58, -2.74, 1.31, 0.01, -2.28, 1.14, -2.49, -2.34, -0.49, 1.64, 0.27, -1.3, 1.21, -1.08, -0.94, -0.09, 0.71, -0.08, -0.92, 0.81, -0.09, -1.36, -0.34, 0.33, 0.84, 0.34, 1.02, -0.7, 0.84, 0.01, 1.49, 1.73, 1.33, 1.52, -0.92, -1.88, 0.7, 0.53, -0.8, -2.81, 0.1, -1.08, 1.28, -1.16, 1.06, 1.89, 4.13, -2.45, -0.54, -0.71, -1.39, 0.6, 0.53, -0.2, -1.99, -0.45, 0.51, 0.94, -0.5, 0.53, -0.96, 0.41, 0.5, 1.96, 2.14, -1.6, -1.67, 0.33, 1.81, -1.7, -1.54, -1.39, -0.71, 0.77, -1.62, -0.12, 0.97, 0.61, -0.4, 0.01, -1.84, -0.35, 0.3, -0.14, -0.37, 0.1, -0.18, 1.1, 0.39, 1.06, 0.97, 0.11, -0.36, -1.3, 1.55, 0.06, -0.77, 0.32, -0.38, 0.17, 0.83, 0.58, -1.25, 1.48, -2.41, -1.39, 0.11, -0.73, 0.73, 0.21, -0.71, -0.15, 0.1, -0.18, 0.46, -0.26, 0.46, -0.31, -0.42, -0.41, -0.16, -0.16, 0.33, 0.74, 0.37, -0.06, 0.74, 0.63, -0.55, -0.27, 0.28, -0.0, 0.4, 0.28, 0.38, 0.73, 0.25, 0.78, 0.11, -0.47, -0.57, -0.37, 0.06, 1.31, 0.66, -1.01, 0.76, -1.58, -0.17, -1.36, -0.04, 0.56, 0.76, 0.42, -0.66, -0.19, -0.79, -0.03, 0.73, -0.88, -0.46, -1.24, -0.38, 0.0, -0.28, 0.24, -0.03, -0.17, -0.13, 0.28, 0.7, -0.52, 0.17, 0.0, 0.47, 0.71, 0.82, -0.85, -0.05, 0.55, 0.07, 0.34, -0.01, 0.02, -0.07, 0.09, 0.35]\n",
      "[-0.3, -0.38, 0.83, -1.2, -3.14, 2.58, 1.44, -0.98, 0.6, 2.15, 1.36, 0.76, -1.53, 0.56, -1.03, 3.05, -1.69, 0.75, -2.99, 1.02, -0.25, -1.51, -1.25, -1.37, 0.94, -0.8, -1.46, -1.22, 1.68, 2.95, -3.04, 0.72, -0.88, 0.25, 2.65, 2.65, -1.42, -1.61, -1.32, 2.14, -1.13, 1.11, -2.13, -2.74, -1.33, -4.08, 1.01, -3.9, 0.31, 1.84, -0.07, 0.49, -2.18, 2.42, -3.0, -0.04, 1.74, -0.78, -1.17, -1.08, -2.37, -0.73, 0.54, 0.82, 2.01, 2.26, -1.69, -1.21, -0.88, 0.32, 0.4, -1.56, -1.55, 0.01, -1.91, -2.15, -0.78, 1.92, 1.47, -1.11, 0.14, 0.1, -0.38, -1.96, 0.16, -2.08, -2.66, 3.03, 6.1, 1.61, -2.65, -0.73, -0.44, 4.52, 2.04, 1.1, 1.63, 0.92, -0.65, -0.09, 2.1, -0.2, 2.68, -0.38, -0.28, -1.36, 0.32, 1.37, 0.27, 1.03, 0.0, 0.76, 3.53, 3.08, 1.58, -0.91, -0.77, 1.34, -3.87, -1.27, -0.49, 2.06, 0.44, 0.02, 0.36, -1.32, 0.39, -0.45, -0.48, 0.79, -1.18, 1.06, 1.51, 1.97, 2.2, 2.36, -0.07, -2.3, 1.2, -0.41, -0.16, 0.41, 1.06, 0.31, 0.49, 1.14, 1.71, 0.57, 1.72, -0.3, 1.85, 2.09, 3.14, 1.52, -1.08, 6.21, 0.42, 1.77, 2.49, 1.34, 1.07, 1.71, 0.05, -3.61, -2.19, 1.59, 1.2, 0.56, -0.7, -2.05, -0.07, 2.16, -5.11, -1.27, -0.55, 1.92, 0.66, 3.9, -0.51, 0.33, -1.87, 0.24, -1.63, 2.72, 1.42, -0.52, 1.74, -1.66, -2.6, -0.67, 2.56, -1.67, -1.39, -0.64, 1.37, -0.87, 2.51, 1.94, 1.69, -0.95, -0.68, -0.99, 1.16, 2.56, 2.58, 0.09, -1.68, -1.24, 2.59, -1.31, -0.78, 3.28, -0.48, -0.75, -1.39, 3.1, -0.66, 1.07, -2.52, 1.48, -0.13, -1.74, -0.67, 1.97, -4.15, 1.25, 2.12, -2.9, 0.55, -0.06, 0.55, 1.46, 1.09, 0.04, 4.57, 1.86, -2.52, 0.95, -1.07, 5.37, 0.47, 2.0, -2.05, 1.8, -0.19, 0.14, -3.42, 1.7, -1.92, 1.53, 1.76, 1.21, 0.38, -1.4, 1.04, 5.7, -0.07, -0.81, 1.16, 2.35, 2.76, -3.11, 0.0, 2.33, 1.87, 2.61, 1.83, -3.02, 0.55, -1.38, -0.24, -0.85, 0.1, -0.18, 3.34, 0.29, -1.29, -0.52, -0.49, -0.57, 0.49, -0.78, 2.67, 0.72, 0.03, 2.97, 4.28, 2.31, 4.83, -1.23, -0.07, -2.47, 0.0, -1.69, -0.06, 2.2, -1.98, -0.34, 0.54, 1.96, -1.92, -1.98, 2.52, 0.81, 0.55, -1.72, -0.13, 0.89, -0.98, 0.82, -1.51, -1.51, -1.32, -0.61, -2.2, -3.25, 1.64, 1.04, -1.85, 3.61, 0.44, -1.76, 0.77, -2.78, -6.07, 0.0, 0.86, -2.04, 1.68, 3.53, -1.32, -1.45, -0.37, -0.89, -0.92, 0.55, 1.24, -3.31, 1.74, 3.48, 6.4, -3.07, -0.29, -2.71, -1.6, 3.68, 1.1, 2.86, 1.94, 1.98, 3.97, -1.89, 2.22, -1.67, 3.66, 16.41, -0.33, 3.62, 1.97, -3.31, -0.82, 2.65, 0.62, 0.39, 2.34, 0.37, 2.09, -0.7, 0.7, -0.04, 1.27, 0.65, 1.57, 0.45, 0.74, -1.16, 0.83, 0.97, -4.11, -0.35, -2.74, 1.48, -3.75, 2.31, 0.57, -0.26, -0.79, -2.82, -2.5, 5.0, -3.01, 4.82, -1.03, -1.25, -0.73, 1.52, -0.67, -0.95, -0.86, -0.76, -2.94, 4.36, 0.76, 0.69, 1.66, -2.78, -0.47, -3.54, -3.57, 2.5, -2.57, -1.06, -3.77, -1.11, -2.04, 0.11, 1.29, -0.22, 1.89, 2.01, 1.74, 1.99, 2.69, -3.66, 2.17, 3.71, -3.46, -2.51, -2.64, 1.82, 2.46, 5.67, -0.15, 1.3, -4.48, 2.22, -0.36, -2.96, -0.68, -5.33, -3.12, 0.0, 0.16, 3.13, 0.55, 5.76, 2.05, 1.72, 2.52, 2.52, 4.28, 3.87, -1.53, -1.53, 2.66, -1.2, -1.61, 2.28, -3.15, 2.53, 2.79, -0.89, -1.2, -1.66, -1.34, -2.94, 0.94, 1.21, -0.37, -2.77, 1.11, -0.99, -1.48, 2.6, 6.95, 1.58, 2.67, 4.99, 1.16, 1.83, 0.39, -2.59, 0.34, -4.23, 0.87, 5.37, -0.35, 1.85, 3.19, -2.82, 2.72, 1.36, -3.46, -0.4, 5.08, 0.0, 6.38, 3.09, -3.08, -2.89, -3.87, -2.52, 2.45, 3.42, 0.42, -1.09, 7.07, -2.71, 3.23, -2.9, -2.93, -4.35, -1.93, 0.51, 4.55, -0.52, -0.08, 7.24, -2.51, -1.28, 0.77, 4.29, 1.52, -9.03, 1.97, -1.93, 4.26, -6.34, -0.49, -2.96, 2.14, 5.15, -3.05, -0.75, -9.29, -2.87, -3.73, 2.52, -2.26, -2.72, -3.2, 5.87, -1.35, -2.22, 6.05, -2.5, -7.5, -2.29, 5.1, -2.33, -0.55, 1.97, -1.27, 2.17, 0.41, -3.21, -1.31, -3.82, 0.3, 0.84, -2.25, -0.22, 4.55, 1.85, -1.42, -0.12, 1.07, 4.45, -2.56, 2.58, 1.58, 6.81, 0.95, -4.06, -1.73, -1.46, -0.79, 7.82, -9.24, -5.3, -7.31, 0.12, -4.4, 2.18, 0.4, 16.35, -4.43, -1.55, -2.36, -1.16, -0.14, 4.66, -1.73, -7.03, -7.93, 3.56, 2.93, 1.5, 5.86, -4.27, -3.53, 2.6, 5.83, 3.04, -3.62, -3.52, -1.83, 3.92, -3.93, -1.23, -0.92, -6.23, -2.1, -1.08, -2.16, 2.97, 2.7, -1.17, -2.51, -2.37, 0.43, 3.46, -1.41, 0.87, -0.61, -0.14, -1.1, 2.05, 2.55, 5.4, 3.25, 6.67, -1.44, 2.49, 5.7, -1.24, -1.98, -2.34, 1.33, -3.35, 7.07, -4.2, 4.3, -8.69, -3.18, -3.31, 2.3, 0.19, 0.71, -3.09, 0.68, 1.06, -2.76, 0.48, -3.33, 3.56, -0.09, -3.39, -0.87, -2.53, -0.65, -0.59, 0.86, -3.27, 0.66, -1.51, -0.84, -3.74, -0.56, 0.26, -1.07, 1.09, -2.44, -0.32, -1.28, 0.94, 0.64, 0.32, 0.84, -2.96, -0.64, 0.57, 3.71, -0.65, -1.61, 2.47, 0.2, -3.22, -1.54, 1.03, 2.57, 0.29, -0.07, 4.77, 2.34, -0.38, 3.19, -1.33, -2.23, -0.47, -1.8, 0.62, -1.91, 1.14, -2.18, 1.6, 1.6, -1.68, -2.44, 0.39, -2.1, 2.74, -2.26, 5.13, 0.66, -1.04, -0.22, 0.0, -0.62, -1.36, -0.3, -0.76, -0.46, -2.13, 1.23, 6.35]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "719"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_spy = \"Stocks/SPY.csv\"\n",
    "\n",
    "stock = \"UBER\"\n",
    "\n",
    "\n",
    "path_stock = f\"Stocks/{stock}.csv\"\n",
    "\n",
    "\n",
    "\n",
    "SPY_array = []\n",
    "stock_array = []\n",
    "\n",
    "with open(path_spy, mode='r', newline='') as spy:\n",
    "    with open(path_stock, mode='r', newline='') as stock:\n",
    "        spy_reader = csv.reader(spy)\n",
    "        stock_reader = csv.reader(stock)\n",
    "        while(True):\n",
    "            spy_reader = next(csv.reader(spy))[1]\n",
    "            stock_reader = next(csv.reader(stock))[1]\n",
    "            if stock_reader == \"--\":\n",
    "                break\n",
    "            SPY_array.append(round(float(spy_reader) , 2))\n",
    "            stock_array.append(round(float(stock_reader) , 2))\n",
    "                \n",
    "            \n",
    "print(SPY_array)    \n",
    "print(stock_array)\n",
    "\n",
    "len(stock_array)-7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# aim frist 80 percent of stocks go into train\n",
    "# recent 20 percent used for test\n",
    "\n",
    "num = 2000\n",
    "x_train = np.zeros((num, 14))\n",
    "y_train = np.zeros(num)\n",
    "x_test = np.zeros((len(stock_array) - int(len(stock_array)*.8), 14))\n",
    "y_test = np.zeros(len(stock_array) - int(len(stock_array)*.8))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num):\n",
    "    random_num = random.randint(0, int(len(stock_array)*.8) - 8)\n",
    "    x_train[i, :7] = SPY_array[random_num:random_num + 7]\n",
    "    x_train[i, 7:14] = stock_array[random_num:random_num + 7]\n",
    "    if (stock_array[random_num+8] > 0.0): \n",
    "        y_train[i] = 1\n",
    "    else:\n",
    "        y_train[i] = 0\n",
    "for i in range(int(len(stock_array) *.8), len(stock_array)):\n",
    "    k = i - int(len(stock_array) *.8)\n",
    "    x_test[k, :7] = SPY_array[k:k + 7]\n",
    "    x_test[k, 7:14] = stock_array[k:k + 7]\n",
    "    if (stock_array[k+8] > 0.0): \n",
    "        y_test[k] = 1\n",
    "    else:\n",
    "        y_test[k] = 0\n",
    "    \n",
    "\n",
    "# print(x_train.shape)   # train on x percent of stocks?\n",
    "# print(x_train)\n",
    "# print(y_train.shape)\n",
    "# print(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (10000, 14)\n",
      "Shape of y_train: (10000,)\n",
      "Shape of x_test: (2000, 14)\n",
      "Shape of y_test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# num_samples = 10000\n",
    "# num_features = 14\n",
    "# num_classes = 3\n",
    "\n",
    "# x_train = np.random.rand(num_samples, num_features)\n",
    "# y_train = np.random.randint(num_classes, size=num_samples)\n",
    "\n",
    "# x_test = np.random.rand(int(num_samples * 0.2), num_features)\n",
    "# y_test = np.random.randint(num_classes, size=int(num_samples * 0.2))\n",
    "\n",
    "# print(\"Shape of x_train:\", x_train.shape)\n",
    "# print(\"Shape of y_train:\", y_train.shape)\n",
    "# print(\"Shape of x_test:\", x_test.shape)\n",
    "# print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(14,)),   # changed 100 from num samples \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 2s 8ms/step - loss: 0.6715 - accuracy: 0.5725 - val_loss: 0.6741 - val_accuracy: 0.5685\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6033 - accuracy: 0.6795 - val_loss: 0.6544 - val_accuracy: 0.6644\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5584 - accuracy: 0.7355 - val_loss: 0.6394 - val_accuracy: 0.6438\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5230 - accuracy: 0.7535 - val_loss: 0.6101 - val_accuracy: 0.7123\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7985 - val_loss: 0.5817 - val_accuracy: 0.7466\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8195 - val_loss: 0.5684 - val_accuracy: 0.7123\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.8380 - val_loss: 0.5430 - val_accuracy: 0.7534\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8565 - val_loss: 0.5184 - val_accuracy: 0.7603\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3553 - accuracy: 0.8640 - val_loss: 0.4977 - val_accuracy: 0.7603\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8810 - val_loss: 0.4742 - val_accuracy: 0.8014\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8975 - val_loss: 0.4554 - val_accuracy: 0.8151\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.9140 - val_loss: 0.4368 - val_accuracy: 0.8082\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.9280 - val_loss: 0.4018 - val_accuracy: 0.8630\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2276 - accuracy: 0.9380 - val_loss: 0.3889 - val_accuracy: 0.8630\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9420 - val_loss: 0.3650 - val_accuracy: 0.8836\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9510 - val_loss: 0.3366 - val_accuracy: 0.8767\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9555 - val_loss: 0.3079 - val_accuracy: 0.9178\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.9690 - val_loss: 0.2992 - val_accuracy: 0.9315\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9760 - val_loss: 0.2828 - val_accuracy: 0.9110\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9795 - val_loss: 0.2564 - val_accuracy: 0.9384\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9855 - val_loss: 0.2472 - val_accuracy: 0.9247\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.9885 - val_loss: 0.2157 - val_accuracy: 0.9589\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9905 - val_loss: 0.2162 - val_accuracy: 0.9521\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9915 - val_loss: 0.1972 - val_accuracy: 0.9658\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0682 - accuracy: 0.9945 - val_loss: 0.1777 - val_accuracy: 0.9726\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9975 - val_loss: 0.1745 - val_accuracy: 0.9658\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9980 - val_loss: 0.1716 - val_accuracy: 0.9795\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9975 - val_loss: 0.1730 - val_accuracy: 0.9658\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.9990 - val_loss: 0.1649 - val_accuracy: 0.9658\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.9985 - val_loss: 0.1673 - val_accuracy: 0.9658\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9985 - val_loss: 0.1380 - val_accuracy: 0.9726\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9990 - val_loss: 0.1450 - val_accuracy: 0.9726\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9990 - val_loss: 0.1325 - val_accuracy: 0.9726\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.9995 - val_loss: 0.1427 - val_accuracy: 0.9658\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.9990 - val_loss: 0.1264 - val_accuracy: 0.9726\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.9990 - val_loss: 0.1247 - val_accuracy: 0.9726\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9995 - val_loss: 0.1197 - val_accuracy: 0.9726\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9990 - val_loss: 0.1235 - val_accuracy: 0.9658\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9995 - val_loss: 0.1083 - val_accuracy: 0.9726\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9990 - val_loss: 0.1311 - val_accuracy: 0.9658\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9995 - val_loss: 0.1057 - val_accuracy: 0.9726\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9995 - val_loss: 0.1141 - val_accuracy: 0.9726\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9726\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9995 - val_loss: 0.1157 - val_accuracy: 0.9726\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9726\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9795\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9726\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 0.9726\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1135 - val_accuracy: 0.9726\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.1073 - val_accuracy: 0.9795\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 0.9726\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.9726\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9795\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 0.9726\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 0.9726\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9726\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 0.9726\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1121 - val_accuracy: 0.9726\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9726\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9726\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9726\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9726\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9726\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9726\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9726\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1139 - val_accuracy: 0.9726\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1249 - val_accuracy: 0.9726\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9726\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9726\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1248 - val_accuracy: 0.9726\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 0.9726\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1199 - val_accuracy: 0.9726\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9726\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1231 - val_accuracy: 0.9726\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9726\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9726\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9726\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9726\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9726\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9726\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1274 - val_accuracy: 0.9726\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9726\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1248 - val_accuracy: 0.9726\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 9.6287e-04 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9726\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 9.1912e-04 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9726\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 8.9341e-04 - accuracy: 1.0000 - val_loss: 0.1315 - val_accuracy: 0.9726\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 8.3794e-04 - accuracy: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9726\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 8.0490e-04 - accuracy: 1.0000 - val_loss: 0.1340 - val_accuracy: 0.9726\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 7.6720e-04 - accuracy: 1.0000 - val_loss: 0.1315 - val_accuracy: 0.9726\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 7.3620e-04 - accuracy: 1.0000 - val_loss: 0.1375 - val_accuracy: 0.9726\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.9659e-04 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9726\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.6283e-04 - accuracy: 1.0000 - val_loss: 0.1324 - val_accuracy: 0.9726\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.3157e-04 - accuracy: 1.0000 - val_loss: 0.1368 - val_accuracy: 0.9726\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.1455e-04 - accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9726\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.8287e-04 - accuracy: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.9726\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.5895e-04 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9726\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3296e-04 - accuracy: 1.0000 - val_loss: 0.1342 - val_accuracy: 0.9726\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1352e-04 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.9726\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9523e-04 - accuracy: 1.0000 - val_loss: 0.1403 - val_accuracy: 0.9726\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.6130e-04 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19503dc2210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))  # change back the validation_data x_train to x_test same for y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 0.9726\n",
      "Test accuracy: 0.9726027250289917\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.29 -1.93 -0.65  1.87 -0.72 -0.45  1.93 -4.35 -1.93  0.51  4.55 -0.52\n",
      "  -0.08  7.24]]\n",
      "[[-0.44  0.32 -0.28  0.81  0.89 -0.1  -0.44 -3.11  0.    2.33  1.87  2.61\n",
      "   1.83 -3.02]]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[[1.0000000e+00 5.1404543e-09]]\n"
     ]
    }
   ],
   "source": [
    "# running once\n",
    "new_input = np.array([x_train[0]])  # 1 sample, 14 features\n",
    "\n",
    "print(new_input)\n",
    "print(np.array([x_train[1]]))\n",
    "\n",
    "#new_input = np.array(x_train[1])\n",
    "# Make a prediction\n",
    "prediction = model.predict(new_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('synthetic_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
