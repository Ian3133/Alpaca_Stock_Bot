{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.0)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading numpy-1.24.3-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.56.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Downloading numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.8 MB 330.3 kB/s eta 0:00:45\n",
      "   ---------------------------------------- 0.1/14.8 MB 819.2 kB/s eta 0:00:18\n",
      "    --------------------------------------- 0.3/14.8 MB 1.9 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.5/14.8 MB 2.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/14.8 MB 3.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.0/14.8 MB 3.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.3/14.8 MB 3.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.5/14.8 MB 3.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.7/14.8 MB 3.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.9/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.0/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.3/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.4/14.8 MB 3.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.8/14.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.9/14.8 MB 4.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.2/14.8 MB 4.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.4/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.6/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.9/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.4/14.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.6/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.8/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.0/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.2/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.4/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.7/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.9/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.1/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.4/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.6/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.8/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.0/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.2/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.4/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.5/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.8/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.9/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.2/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.5/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.6/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.0/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.1/14.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.3/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.7/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.9/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.2/14.8 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.3/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.4/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.7/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.0/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.2/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.5/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.6/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.0/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.2/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.4/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.7/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.8/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.2/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.4/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.5/14.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.8/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.0/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.3/14.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.8/14.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.8/14.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.8/14.8 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: typing-extensions, numpy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.1\n",
      "    Uninstalling numpy-1.25.1:\n",
      "      Successfully uninstalled numpy-1.25.1\n",
      "Successfully installed numpy-1.24.3 typing-extensions-4.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.7.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.18.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np  \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Alpaca_Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stock \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAlpaca_Data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mipynb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_data\n\u001b[0;32m      4\u001b[0m create_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1050\u001b[39m)   \u001b[38;5;66;03m# 700 works  1050 works\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspy done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Alpaca_Data'"
     ]
    }
   ],
   "source": [
    "stock = \"AMD\"\n",
    "#from Alpaca_Data.ipynb import create_data\n",
    "\n",
    "# create_data(\"SPY\", 1050)   # 700 works  1050 works\n",
    "# print(\"spy done\")\n",
    "# create_data(\"\", 1050) \n",
    "# print(\"both good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24, 0.93, 0.35, -0.13, -0.05, 0.54, 0.47, 0.07, -0.06, 0.73, 0.37, -0.23, 0.72, -0.36, 0.08, -0.09, 0.3, -1.31, -0.15, 0.39, 0.09, 0.12, -0.22, 0.75, 0.42, -0.32, -0.19, 0.54, 0.37, -0.08, 0.55, 0.03, 0.17, -0.21, -1.29, -1.29, -0.01, 0.38, 0.86, -0.23, 0.77, 0.38, -0.86, -0.49, -1.09, -0.28, -2.07, -0.68, 0.45, 0.12, -0.23, -0.08, 0.77, -2.0, 0.33, 0.12, -0.32, -0.03, 0.28, -0.46, -0.01, -0.17, -0.23, 0.91, 0.7, -0.22, -0.07, -0.39, -0.22, 0.65, 0.16, -0.73, 0.33, -0.16, -0.6, 0.05, 0.76, 0.0, 0.18, 0.05, -0.45, -0.28, 0.69, 0.36, -0.19, -0.44, 0.54, 0.36, -0.09, -0.04, 0.47, 0.04, 0.36, 0.09, -0.23, 0.96, 0.94, -1.17, 0.07, 0.73, -0.04, 0.09, -0.5, 0.18, -0.12, 1.0, 0.52, 0.1, -0.07, -0.24, -0.26, 0.51, 0.43, 1.32, 0.09, -0.22, -0.35, 0.1, -0.25, -0.04, 0.23, 0.33, -0.04, 0.29, -1.2, 0.49, 0.21, -0.03, -0.1, 1.29, 0.54, 0.5, 0.6, 0.29, -0.88, 0.29, 0.24, 0.73, 0.2, -0.56, 0.19, -0.04, 0.05, 0.01, 0.02, 0.83, 0.12, 0.22, -0.1, 0.54, 0.22, 1.06, -1.05, -0.07, 0.28, 0.05, 0.36, 0.98, 0.83, 0.49, 0.49, -0.85, -0.94, -1.03, 0.23, 0.2, -1.12, -1.05, -0.92, 0.74, 0.51, -0.85, -0.85, -0.75, 0.16, 0.37, 1.1, 1.79, 0.03, 0.61, -0.82, 0.16, -0.97, 0.71, -0.24, -0.75, 0.71, -0.47, -0.99, -1.21, 0.01, 0.13, -0.84, 0.29, 0.06, -0.21, 0.05, 0.14, 0.39, -0.49, -0.33, -0.44, -0.29, 0.33, 1.47, 0.12, 0.29, -1.75, 0.86, -0.69, 0.41, 0.72, -1.1, -0.64, -0.76, 0.77, 0.38, -0.51, -0.73, 0.15, 0.45, -0.87, 0.18, -0.69, 0.05, 0.08, 0.23, -1.42, 0.23, 0.33, 0.18, -0.39, -0.44, 0.04, 0.82, 0.38, -0.27, 0.37, -0.08, 0.46, 0.34, -0.02, 0.05, 0.28, 0.2, 0.42, 0.49, 0.31, 0.88, -0.27, 0.06, 0.59, -0.28, -0.06, -0.8, 1.44, 0.04, 0.31, 0.67, -0.01, 0.59, -0.44, 0.32, -0.28, 0.81, 0.89, -0.1, -0.44, 1.13, -0.02, -0.32, -0.72, 0.04, -0.37, 1.04, 0.7, -0.39, 0.19, -0.44, 0.04, -0.25, -0.05, -0.06, 0.91, -0.44, -0.81, -0.95, 0.01, 1.08, 1.33, -0.58, -1.1, 0.16, 0.0, 0.16, 0.47, -0.33, 0.38, -0.08, 1.05, -0.93, -0.13, 0.74, 0.59, -0.08, -0.72, 0.51, 1.17, -0.1, 0.36, -0.04, -0.41, -0.41, 1.0, -0.49, -1.66, 0.42, 0.75, -0.82, 2.4, 0.88, 0.31, 0.93, -1.3, -2.05, 0.13, -1.52, -0.14, 1.12, 1.31, -0.17, -0.24, -0.54, 0.24, -0.22, -0.25, -0.98, 0.3, -0.12, 0.88, 0.34, 1.01, 0.54, -1.77, -0.6, 1.55, 0.01, 0.18, 0.46, 1.38, 1.33, -0.55, 0.5, 0.4, 1.11, 0.33, 0.99, 1.48, -0.18, -1.88, -0.18, 1.24, 0.07, 0.84, 0.86, -0.64, 1.43, -0.61, 0.15, -0.92, 0.47, 1.0, -1.22, -0.36, 0.86, -0.61, 0.78, 0.35, -0.9, -0.5, -1.18, -0.55, -2.01, 1.23, -0.42, 0.28, 0.06, -1.4, -1.08, 1.16, -0.34, 3.08, -0.21, -0.8, 0.12, 0.72, 0.82, -0.01, -0.43, 0.97, -0.34, -0.66, -0.39, 0.74, 1.71, -1.53, 0.23, 0.59, -0.17, -0.12, -2.35, -1.44, -0.06, 2.41, -0.81, 0.1, 1.62, 0.79, 2.51, -0.71, -0.13, -1.07, 0.77, -2.96, 4.8, -0.45, -0.14, -1.08, -1.67, -0.64, 0.99, 1.5, 1.53, -1.28, -1.1, 1.69, -1.26, -0.57, -0.71, -0.63, -2.26, -0.25, 1.65, 0.37, -0.72, 0.03, -2.17, 0.54, 0.96, 1.26, 1.26, 1.88, -0.6, -2.01, 0.9, -1.19, -1.4, 0.11, -3.36, 1.03, 0.38, -0.13, -0.89, -0.67, 0.24, 0.17, 0.46, 0.96, 1.2, -0.71, 0.29, -0.21, -0.54, 0.93, -0.05, 1.01, -0.26, 0.4, 1.08, 1.03, 1.69, -0.75, -0.05, -0.96, 1.17, 0.59, 1.6, -1.66, 0.67, 1.15, 0.99, -0.74, -0.42, 0.36, 1.0, 0.3, 1.62, 1.24, 0.27, -0.23, -2.45, -0.63, 2.28, 0.38, 1.02, 0.86, 0.1, -1.04, 0.49, -0.79, -1.28, -1.29, -1.93, -0.65, 1.87, -0.72, -0.45, 1.93, -1.34, -0.15, 1.8, 1.67, 1.29, 0.34, 1.04, -0.92, 0.22, -2.88, 0.44, 0.03, 1.26, 0.76, -1.34, -1.34, -1.71, 0.06, -2.53, 2.87, 0.33, 0.58, -2.74, 1.31, 0.01, -2.28, 1.14, -2.49, -2.34, -0.49, 1.64, 0.27, -1.3, 1.21, -1.08, -0.94, -0.09, 0.71, -0.08, -0.92, 0.81, -0.09, -1.36, -0.34, 0.33, 0.84, 0.34, 1.02, -0.7, 0.84, 0.01, 1.49, 1.73, 1.33, 1.52, -0.92, -1.88, 0.7, 0.53, -0.8, -2.81, 0.1, -1.08, 1.28, -1.16, 1.06, 1.89, 4.13, -2.45, -0.54, -0.71, -1.39, 0.6, 0.53, -0.2, -1.99, -0.45, 0.51, 0.94, -0.5, 0.53, -0.96, 0.41, 0.5, 1.96, 2.14, -1.6, -1.67, 0.33, 1.81, -1.7, -1.54, -1.39, -0.71, 0.77, -1.62, -0.12, 0.97, 0.61, -0.4, 0.01, -1.84, -0.35, 0.3, -0.14, -0.37, 0.1, -0.18, 1.1, 0.39, 1.06, 0.97, 0.11, -0.36, -1.3, 1.55, 0.06, -0.77, 0.32, -0.38, 0.17, 0.83, 0.58, -1.25, 1.48, -2.41, -1.39, 0.11, -0.73, 0.73, 0.21, -0.71, -0.15, 0.1, -0.18, 0.46, -0.26, 0.46, -0.31, -0.42, -0.41, -0.16, -0.16, 0.33, 0.74, 0.37, -0.06, 0.74, 0.63, -0.55, -0.27, 0.28, -0.0, 0.4, 0.28, 0.38, 0.73, 0.25, 0.78, 0.11, -0.47, -0.57, -0.37, 0.06, 1.31, 0.66, -1.01, 0.76, -1.58, -0.17, -1.36, -0.04, 0.56, 0.76, 0.42, -0.66, -0.19, -0.79, -0.03, 0.73, -0.88, -0.46, -1.24, -0.38, 0.0, -0.28, 0.24, -0.03, -0.17, -0.13, 0.28, 0.7, -0.52, 0.17, 0.0, 0.47, 0.71, 0.82, -0.85, -0.05, 0.55, 0.07, 0.34, -0.01, 0.02, -0.07, 0.09, 0.35, -0.18]\n",
      "[0.58, 0.09, 1.13, -2.76, -1.98, -0.07, 0.02, -0.2, 1.2, 0.38, -0.36, 0.85, 7.0, -2.41, 1.35, -1.2, 0.5, -2.83, 0.14, -1.06, 0.37, -0.5, -1.64, 2.79, -0.06, 0.14, 0.21, 1.1, 1.18, 0.58, -0.2, -1.14, 0.63, -0.12, -2.03, -2.03, 0.31, -0.26, 0.57, -0.65, 0.63, -2.0, -0.78, -1.59, -0.51, 0.96, -3.55, -0.86, -0.11, 0.39, -0.31, 0.21, 2.85, -4.58, 0.25, 2.18, 0.19, 0.39, -1.97, -0.54, 0.78, -0.5, 0.15, 1.36, 1.1, 0.3, -1.55, -0.55, -0.1, -0.03, 1.11, 1.02, -1.17, -0.29, -4.03, 0.02, 3.22, 2.94, 0.06, -0.69, 1.62, -1.74, 0.57, 1.24, -1.07, -0.78, 0.21, 1.46, 0.26, -1.2, -0.66, 0.92, 0.52, -0.73, 0.65, 0.16, 0.63, -1.04, 0.03, 2.3, 0.24, 0.15, -1.11, -1.45, -1.23, 1.58, 0.53, 1.17, -0.19, 0.26, 1.26, 0.33, 0.71, 3.43, -0.04, -0.23, -0.65, -1.69, -0.71, -0.35, 0.29, 0.26, -0.34, 0.94, -0.54, -0.43, 1.45, 2.58, 0.28, -0.07, 1.85, 0.84, 0.91, 0.04, -0.95, 0.41, -0.85, 4.0, 1.14, 0.76, 0.59, -0.31, 0.01, -0.21, 0.14, 1.67, 0.02, 0.66, -0.6, 1.0, 1.27, 2.02, -1.67, -0.45, 1.12, -0.38, -0.92, 0.87, 1.49, 0.2, 0.14, -0.37, -0.59, -3.02, 0.24, 1.05, -1.75, 0.91, -1.94, 1.62, 1.31, -0.25, -0.25, -1.03, -0.18, -0.37, 1.14, 3.43, 0.0, 0.49, -1.44, 1.45, -1.12, 1.22, -0.13, -1.45, 0.65, -1.38, -0.57, -1.01, 0.61, 0.21, -1.95, -0.29, -1.08, -0.8, -0.31, 0.64, 1.26, 0.98, -0.29, -0.88, -2.87, 1.54, 2.52, 0.6, 1.93, -2.61, 0.54, -1.47, 1.47, 1.92, -1.32, -0.49, -0.74, 2.0, 1.25, 0.03, -2.74, -0.92, 0.35, -0.84, -0.3, -0.67, 0.17, 1.2, -0.18, -1.27, 0.5, 0.07, -0.7, -0.84, -1.27, 0.17, 0.26, -0.54, -0.16, 0.92, 2.42, -0.4, 1.82, -0.09, -0.5, 1.58, 0.57, -0.28, -0.69, 1.8, 0.33, -0.79, -0.19, 1.75, -3.2, 3.99, -0.83, 1.91, 0.82, -3.03, -1.14, 2.19, 2.02, -4.26, 2.06, -0.29, 0.06, 2.25, 1.74, -0.18, 1.24, -1.0, 1.47, -0.53, 0.52, -1.22, 1.82, 1.4, 1.3, -0.21, -0.8, 0.28, -0.04, 2.24, -0.41, 1.88, 0.25, -0.82, -2.11, 0.02, 1.13, 0.91, -0.57, -1.7, -1.78, 1.02, 0.17, 1.33, -0.29, 0.98, 1.46, 1.87, 0.01, -0.09, 0.47, -0.75, -1.36, -0.48, -0.87, 1.16, -0.59, 1.81, 0.59, 1.24, 1.24, 1.91, 0.02, -1.84, 1.51, 0.56, -0.64, 2.18, 2.09, 1.61, 2.64, -2.98, -2.39, -0.19, -0.62, -0.69, -0.87, -3.38, 2.68, 0.66, -0.43, 1.0, -0.71, 0.15, -1.29, 0.08, -0.52, 1.33, 1.03, 1.66, -0.37, 0.19, -0.22, 2.21, 0.63, 0.6, -0.61, 2.44, 2.25, 0.41, -0.32, 4.36, 3.28, 0.16, -0.94, 4.82, 2.01, -2.29, -0.76, 2.26, 1.19, -0.26, 1.12, 3.16, 2.13, -0.94, -1.15, -0.3, 1.52, 2.19, -0.96, 1.63, 0.55, 0.4, 1.83, 0.75, 0.97, -0.5, -0.93, -0.19, -1.29, 1.53, 1.06, 1.32, -1.52, 0.42, -6.21, 0.19, -0.37, 6.3, -0.68, 1.11, 0.84, 3.13, 2.79, -0.8, -2.5, -0.33, -3.05, -0.37, 1.82, 0.79, 3.51, -2.6, 1.39, 2.82, -4.59, -1.79, -5.96, -3.14, 0.29, 2.2, -1.27, 0.07, 2.21, -0.33, 1.7, 1.61, 1.19, -1.87, 0.76, -3.47, 4.66, -0.01, -1.34, -3.31, -0.99, -0.17, 2.65, 2.66, 2.01, -1.49, -1.1, 2.86, 0.35, -0.46, -0.72, 0.43, -2.37, -0.79, 1.41, -0.27, -1.29, 1.07, -0.09, 1.19, 2.52, 3.01, 3.01, 1.32, -0.76, -1.77, -0.87, -2.3, -1.52, -2.48, -5.02, 3.16, 1.89, 0.07, -1.68, -0.78, -0.26, 0.13, 0.02, 1.06, 1.75, -2.81, 0.24, -2.18, -0.7, 1.7, 1.44, 2.22, 1.65, 0.93, 1.21, 1.61, 3.11, -3.75, -2.5, -1.4, 2.6, 3.61, 1.69, -0.7, 2.24, 0.01, 0.66, -3.41, 0.21, 0.83, 1.42, -0.46, 3.67, 2.09, -0.01, -1.1, -4.9, -2.58, 4.61, 2.34, 1.67, 1.08, 2.96, -2.37, 1.09, -2.54, -2.65, -2.82, -0.75, 1.17, 2.99, -3.21, -0.05, 7.66, -1.09, -4.02, 2.76, 1.73, 2.79, -0.53, 0.3, 0.57, 0.24, -3.37, -2.15, -0.38, 2.05, 2.22, -2.56, -0.16, -1.87, 0.12, -4.54, 3.21, 0.8, 1.42, -3.63, 2.39, 2.15, -1.47, 2.88, -3.53, -6.12, -3.45, 2.22, -0.69, -3.52, 1.95, -2.47, 0.64, -0.74, 0.52, -2.14, -3.61, 1.97, -0.11, -1.02, -2.17, 1.22, 2.33, -1.51, 0.77, -1.29, 2.1, -2.05, 4.57, 2.83, 3.48, -1.2, -2.58, -3.35, 0.53, 2.43, -1.05, -3.55, -0.83, -2.68, -2.64, -2.11, 1.32, 1.64, 10.58, -2.96, -0.2, -1.78, -3.98, 0.35, 2.15, 0.29, -4.47, -0.01, 0.41, 0.85, -0.93, 1.97, -2.43, -3.73, -0.11, 4.34, 3.65, -0.63, -3.92, -2.54, 5.01, -1.99, -2.68, -0.53, -1.05, 1.4, -3.73, 0.35, 2.64, 1.83, -0.08, 0.49, -3.6, -3.36, 0.18, -0.13, 0.01, -0.57, -1.65, 1.94, 0.76, -0.24, 1.83, -1.07, 0.67, -2.65, 2.04, -2.45, -0.02, -0.03, -0.65, -0.75, 0.57, 1.03, -1.23, 3.31, -7.28, -3.79, 2.49, -0.1, 1.09, -0.76, -1.66, -1.49, -1.04, 0.08, 0.99, -0.46, 0.71, 0.49, -2.25, -0.09, 0.9, -0.41, 1.5, -0.35, 0.41, 0.42, 0.57, 1.12, -0.47, -0.23, 0.21, 1.23, 0.12, -1.03, 0.08, 0.32, -0.28, 1.41, 1.58, 0.93, 0.72, -1.11, -0.84, 2.08, -0.12, -1.58, 1.38, 0.21, -1.09, -1.03, -0.98, 2.56, 2.91, 0.1, -0.58, 0.64, 0.49, 1.89, 0.64, 0.1, -1.12, -1.46, -0.41, -0.38, -0.5, 1.32, -1.8, 0.77, 0.19, -0.32, -0.5, -0.74, -0.0, -0.86, 1.37, 1.24, 1.57, 1.3, -0.51, -0.39, -0.06, 2.43, -0.98, -2.98, -0.84, 0.55, 2.4, 1.06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_spy = \"Stocks/SPY.csv\"\n",
    "\n",
    "stock = \"UBER\"\n",
    "\n",
    "\n",
    "path_stock = f\"Stocks/{stock}.csv\"\n",
    "\n",
    "\n",
    "\n",
    "SPY_array = []\n",
    "stock_array = []\n",
    "\n",
    "with open(path_spy, mode='r', newline='') as spy:\n",
    "    with open(path_stock, mode='r', newline='') as stock:\n",
    "        spy_reader = csv.reader(spy)\n",
    "        stock_reader = csv.reader(stock)\n",
    "        while(True):\n",
    "            spy_reader = next(csv.reader(spy))[1]\n",
    "            stock_reader = next(csv.reader(stock))[1]\n",
    "            if stock_reader == \"--\":\n",
    "                break\n",
    "            SPY_array.append(round(float(spy_reader) , 2))\n",
    "            stock_array.append(round(float(stock_reader) , 2))\n",
    "                \n",
    "            \n",
    "print(SPY_array)    \n",
    "print(stock_array)\n",
    "\n",
    "len(stock_array)-7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# aim frist 80 percent of stocks go into train\n",
    "# recent 20 percent used for test\n",
    "\n",
    "num = 2000\n",
    "x_train = np.zeros((num, 14))\n",
    "y_train = np.zeros(num)\n",
    "x_test = np.zeros((len(stock_array) - int(len(stock_array)*.8), 14))\n",
    "y_test = np.zeros(len(stock_array) - int(len(stock_array)*.8))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num):\n",
    "    random_num = random.randint(0, int(len(stock_array)*.8) - 8)\n",
    "    x_train[i, :7] = SPY_array[random_num:random_num + 7]\n",
    "    x_train[i, 7:14] = stock_array[random_num:random_num + 7]\n",
    "    if (stock_array[random_num+8] > 0.0): \n",
    "        y_train[i] = 1\n",
    "    else:\n",
    "        y_train[i] = 0\n",
    "for i in range(int(len(stock_array) *.8), len(stock_array)):\n",
    "    k = i - int(len(stock_array) *.8)\n",
    "    x_test[k, :7] = SPY_array[k:k + 7]\n",
    "    x_test[k, 7:14] = stock_array[k:k + 7]\n",
    "    if (stock_array[k+8] > 0.0): \n",
    "        y_test[k] = 1\n",
    "    else:\n",
    "        y_test[k] = 0\n",
    "    \n",
    "\n",
    "# print(x_train.shape)   # train on x percent of stocks?\n",
    "# print(x_train)\n",
    "# print(y_train.shape)\n",
    "# print(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (10000, 14)\n",
      "Shape of y_train: (10000,)\n",
      "Shape of x_test: (2000, 14)\n",
      "Shape of y_test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# num_samples = 10000\n",
    "# num_features = 14\n",
    "# num_classes = 3\n",
    "\n",
    "# x_train = np.random.rand(num_samples, num_features)\n",
    "# y_train = np.random.randint(num_classes, size=num_samples)\n",
    "\n",
    "# x_test = np.random.rand(int(num_samples * 0.2), num_features)\n",
    "# y_test = np.random.randint(num_classes, size=int(num_samples * 0.2))\n",
    "\n",
    "# print(\"Shape of x_train:\", x_train.shape)\n",
    "# print(\"Shape of y_train:\", y_train.shape)\n",
    "# print(\"Shape of x_test:\", x_test.shape)\n",
    "# print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(14,)),   # changed 100 from num samples \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 1s 6ms/step - loss: 0.6653 - accuracy: 0.6075 - val_loss: 0.6543 - val_accuracy: 0.6164\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6059 - accuracy: 0.6720 - val_loss: 0.6317 - val_accuracy: 0.6233\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.7125 - val_loss: 0.6045 - val_accuracy: 0.6986\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5261 - accuracy: 0.7450 - val_loss: 0.5861 - val_accuracy: 0.6918\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7770 - val_loss: 0.5562 - val_accuracy: 0.7260\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.8175 - val_loss: 0.5367 - val_accuracy: 0.7603\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8315 - val_loss: 0.5134 - val_accuracy: 0.7534\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3775 - accuracy: 0.8725 - val_loss: 0.4771 - val_accuracy: 0.7671\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3470 - accuracy: 0.8900 - val_loss: 0.4509 - val_accuracy: 0.8014\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.9030 - val_loss: 0.4388 - val_accuracy: 0.8356\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.9185 - val_loss: 0.4020 - val_accuracy: 0.8699\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.9385 - val_loss: 0.3788 - val_accuracy: 0.8767\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2311 - accuracy: 0.9470 - val_loss: 0.3778 - val_accuracy: 0.8562\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9555 - val_loss: 0.3369 - val_accuracy: 0.8767\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.9575 - val_loss: 0.3213 - val_accuracy: 0.8904\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9645 - val_loss: 0.2958 - val_accuracy: 0.9178\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1542 - accuracy: 0.9680 - val_loss: 0.2892 - val_accuracy: 0.9110\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.9720 - val_loss: 0.2714 - val_accuracy: 0.9110\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.9780 - val_loss: 0.2500 - val_accuracy: 0.9315\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9790 - val_loss: 0.2469 - val_accuracy: 0.9315\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9840 - val_loss: 0.2232 - val_accuracy: 0.9384\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9875 - val_loss: 0.2197 - val_accuracy: 0.9384\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 0.9895 - val_loss: 0.2087 - val_accuracy: 0.9589\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9925 - val_loss: 0.2029 - val_accuracy: 0.9521\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9950 - val_loss: 0.1823 - val_accuracy: 0.9589\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9955 - val_loss: 0.1834 - val_accuracy: 0.9589\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9960 - val_loss: 0.1665 - val_accuracy: 0.9589\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9955 - val_loss: 0.1603 - val_accuracy: 0.9658\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9965 - val_loss: 0.1542 - val_accuracy: 0.9658\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0388 - accuracy: 0.9970 - val_loss: 0.1496 - val_accuracy: 0.9658\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0343 - accuracy: 0.9980 - val_loss: 0.1461 - val_accuracy: 0.9658\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9990 - val_loss: 0.1365 - val_accuracy: 0.9726\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.9985 - val_loss: 0.1462 - val_accuracy: 0.9726\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9726\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.1327 - val_accuracy: 0.9726\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.1289 - val_accuracy: 0.9726\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.1298 - val_accuracy: 0.9726\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 0.9726\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.1261 - val_accuracy: 0.9726\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 0.9726\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.1236 - val_accuracy: 0.9726\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9726\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.1240 - val_accuracy: 0.9726\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9726\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.1261 - val_accuracy: 0.9726\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9726\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9726\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9726\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 0.9726\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1292 - val_accuracy: 0.9726\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9726\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9726\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.1318 - val_accuracy: 0.9726\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9726\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.1319 - val_accuracy: 0.9726\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1329 - val_accuracy: 0.9726\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.1327 - val_accuracy: 0.9726\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9726\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1360 - val_accuracy: 0.9726\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1367 - val_accuracy: 0.9726\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1377 - val_accuracy: 0.9726\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.9726\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.9726\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9726\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1392 - val_accuracy: 0.9726\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.9726\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1501 - val_accuracy: 0.9726\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 0.9726\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1448 - val_accuracy: 0.9726\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1458 - val_accuracy: 0.9726\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9726\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 0.9726\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1453 - val_accuracy: 0.9726\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.9726\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9726\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.9726\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9726\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.9726\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1533 - val_accuracy: 0.9726\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9726\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9726\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 9.8847e-04 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9726\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.3621e-04 - accuracy: 1.0000 - val_loss: 0.1602 - val_accuracy: 0.9726\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.8509e-04 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9726\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 8.4427e-04 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9726\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.9674e-04 - accuracy: 1.0000 - val_loss: 0.1619 - val_accuracy: 0.9726\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 7.6230e-04 - accuracy: 1.0000 - val_loss: 0.1620 - val_accuracy: 0.9726\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 7.2674e-04 - accuracy: 1.0000 - val_loss: 0.1627 - val_accuracy: 0.9726\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.9605e-04 - accuracy: 1.0000 - val_loss: 0.1637 - val_accuracy: 0.9726\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.7264e-04 - accuracy: 1.0000 - val_loss: 0.1633 - val_accuracy: 0.9726\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.3080e-04 - accuracy: 1.0000 - val_loss: 0.1667 - val_accuracy: 0.9726\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.0439e-04 - accuracy: 1.0000 - val_loss: 0.1653 - val_accuracy: 0.9726\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.7586e-04 - accuracy: 1.0000 - val_loss: 0.1645 - val_accuracy: 0.9726\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.5488e-04 - accuracy: 1.0000 - val_loss: 0.1683 - val_accuracy: 0.9726\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2616e-04 - accuracy: 1.0000 - val_loss: 0.1713 - val_accuracy: 0.9726\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0243e-04 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9726\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8160e-04 - accuracy: 1.0000 - val_loss: 0.1684 - val_accuracy: 0.9726\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6536e-04 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9726\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4056e-04 - accuracy: 1.0000 - val_loss: 0.1718 - val_accuracy: 0.9726\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2187e-04 - accuracy: 1.0000 - val_loss: 0.1719 - val_accuracy: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17b1a9c0450>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))  # change back the validation_data x_train to x_test same for y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9726\n",
      "Test accuracy: 0.9726027250289917\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('synthetic_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
